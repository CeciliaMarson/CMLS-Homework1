{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1_help.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8joDs8zGRlCW",
        "0aJ8ynr3SyR-",
        "mU9fWUgbUCy1",
        "6nt-q4jTaYOQ",
        "of46eVQldRuq"
      ],
      "authorship_tag": "ABX9TyMLlt4AwHy7B+EbRzKUDoWJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CeciliaMarson/CMLS-Homework1/blob/master/HW1_help.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8joDs8zGRlCW",
        "colab_type": "text"
      },
      "source": [
        "##Clara's Links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnmJxodDSAwX",
        "colab_type": "text"
      },
      "source": [
        "####Choose random samples for train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJMczlkURZcC",
        "colab_type": "text"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKAzsLumSScD",
        "colab_type": "text"
      },
      "source": [
        "####Pandas Cheat Sheet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBaWr2bQSerQ",
        "colab_type": "text"
      },
      "source": [
        "http://datacamp-community-prod.s3.amazonaws.com/dbed353d-2757-4617-8206-8767ab379ab3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2Ti6FUxSgVu",
        "colab_type": "text"
      },
      "source": [
        "####Scikit Cheat Sheet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi8-76xOSsPb",
        "colab_type": "text"
      },
      "source": [
        "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aJ8ynr3SyR-",
        "colab_type": "text"
      },
      "source": [
        "##Magic Formulas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU9fWUgbUCy1",
        "colab_type": "text"
      },
      "source": [
        "####SelectKBest()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC_p5U5PXwzA",
        "colab_type": "text"
      },
      "source": [
        "it takes as a parameter a score function, which must be applicable to a pair (\n",
        "X, y). The score function must return an array of scores, one for each feature ùëã[:,ùëñ]\n",
        "X\n",
        "[\n",
        ":\n",
        ",\n",
        "i\n",
        "]\n",
        " of\n",
        "X\n",
        " (additionally, it can also return p-values, but these are neither needed nor required). SelectKBest then simply retains the first \n",
        "k\n",
        " features of\n",
        "X\n",
        " with the highest scores.\n",
        "\n",
        "So, for example, if you pass chi2 as a score function, SelectKBest will compute the chi2 statistic between each feature of \n",
        "X\n",
        " and \n",
        "y\n",
        " (assumed to be class labels). A small value will mean the feature is independent of \n",
        "y\n",
        ". A large value will mean the feature is non-randomly related to \n",
        "y\n",
        ", and so likely to provide important information. Only \n",
        "k\n",
        " features will be retained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt2INqJ-TSd6",
        "colab_type": "text"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nt-q4jTaYOQ",
        "colab_type": "text"
      },
      "source": [
        "####cros_val_predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7OpYcTHc5iQ",
        "colab_type": "text"
      },
      "source": [
        "The best explanation of this I found it here: [cros_val_predict()](https://github.com/ageron/handson-ml/issues/435) and is the following description..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ6N3VvMcwD1",
        "colab_type": "text"
      },
      "source": [
        "An example will make things clearer. Suppose there are 10 instances in the dataset and we use 2 folds of 5 instances each. Both cross_val_predict() and cross_val_score() will train two models, one on fold 1 and the other on fold 2. Both functions will use the model 1 to make predictions on fold 2, and model 2 to make predictions on fold 1. This gives two groups of predictions, P1 and P2, with 5 predictions each, for example:\n",
        "\n",
        "P1=[9, 5, 4, 6, 2] and P2=[4, 5, 4, 3, 4].\n",
        "\n",
        "Now suppose the labels are L=[9, 5, 4, 6, 2, 3, 6, 3, 2, 3].\n",
        "You can see that the predictions P1 are perfect, while the predictions P2 are off by 1. Apparently, the first model was perfect, while the second was not.\n",
        "\n",
        "Now cross_val_score() would compute the score independently for each group of predictions. Suppose we are using the MSE, this gives us: MSE(P1)=0 and MSE(P2)=(1^2+1^2+1^2+1^2)/4=1.\n",
        "This is what cross_val_score() would report: [0, 1]. You can then compute the mean and get the final evaluation: 0.5.\n",
        "\n",
        "Now let's look at cross_val_predict(). It would just return the concatenation of P1 and P2, losing the information about which prediction came from which model: [9, 5, 4, 6, 2, 4, 5, 4, 3, 4]. You can then compute the MSE, which is: (0^2+0^2+0^2+0^2+0^2+1^2+1^2+1^2+1^2+1^2)/10 = 0.5.\n",
        "As you can see, you get the same final evaluation in this case. The benefit of cross_val_predict() is that you have access to the predictions, so you can plot them, analyze them, or use them to train blending models (see chapter 7). However, it hides the fact that some models were perfect while others were much worse.\n",
        "\n",
        "If the metric is a simple mean over instance errors (e.g., mean squared error, or mean absolute error, or mean cross-entropy, etc.), then the final scores should always be the same. However, it's not always that simple. For example, consider the precision metric for a classification task (chapter 3). Suppose you are training a binary classifier, and the predictions are: P1=[1, 1, 1, 0, 1] and P2=[0, 0, 1, 1, 0].\n",
        "Now suppose the labels are L=[1, 0, 1, 0, 1, 0, 1, 0, 1, 0]. The precision is the number of true positives divided by the number of positive predictions. So the precision over P1 is 3/4=75%, while the precision over P2 is 1/2=50%. So cross_val_score() will return [0.75, 0.5]. If you compute the mean, you get: 62.5%.\n",
        "Now cross_val_predict() will just return the concatenated predictions P=[1, 1, 1, 0, 1, 0, 0, 1, 1, 0]. If you compute the precision, you get: 4/6=66.66%. That's a different result!\n",
        "\n",
        "Should you trust cross_val_score() or cross_val_predict() then? Well, cross_val_score() is definitely preferable in this case, as it gives you more details about the variability of the metric depending on the trained model, while cross_val_predict() just fuses all the predictions from all folds, although they were made with different models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16HBr4d8dQ2D",
        "colab_type": "text"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of46eVQldRuq",
        "colab_type": "text"
      },
      "source": [
        "####StratifiedKFold()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-mo-6Xod9Uh",
        "colab_type": "text"
      },
      "source": [
        "Similar to what Clara sends us of choosing random samples for our train and test sets. It shuffles your data, after that splits the data into n_splits parts and Done. Now, it will use each part as a test set. Note that it only and always shuffles data one time before splitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meAmp0xwdY_0",
        "colab_type": "text"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html"
      ]
    }
  ]
}